---
title: "Senior Data Science Project"
format: html
---

Introduction: 

I want to create a tool that analyzes an image and generates a short caption describing it. 
This tool can help visually impaired individuals understand pictures and automate tasks 
like tagging photos or creating captions for social media. I plan to use a dataset such as 
Flickr8k, which contain thousands of images paired with captions, to train the 
model.

Project Deliverables: 

• Cleaned Data: Get the dataset ready by organizing pictures and captions. 
• Working Model: A part that looks at pictures (CNN) and a part that writes captions 
(LSTM/Transformer). 
• Performance Check: Test how good the captions are  
• Interactive Demo: A tool where someone can upload a picture, and it shows the caption. 
• Visuals: Simple graphs to explain how the model works and where it focuses in the image. 
• Write-Up: A short explanation of how the project works and why it’s useful.

```{python}
import numpy as np
import pandas as pd
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
import matplotlib.pyplot as plt
from textwrap import wrap
from tqdm import tqdm
from wordcloud import WordCloud
import torch
from transformers import CLIPProcessor, CLIPModel

image_path = '/Users/scotttow123/Documents/BYUI/Senior Data Science Project/Images'
caption_file = "/Users/scotttow123/Documents/BYUI/Senior Data Science Project/captions.txt"

try:
    data = pd.read_csv(caption_file)
    print("Data loaded successfully")
except FileNotFoundError:
    print(f"Error: The file {caption_file} was not found.")
    data = pd.DataFrame() 

data.head()
```

```{python}
if data.duplicated().any():
    print(f"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...")
    data = data.drop_duplicates()

if data['caption'].isnull().any():
    print(f"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.")
    data['caption'] = data['caption'].fillna("No caption")

valid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))
if not valid_image_paths.all():
    print(f"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...")
    data = data[valid_image_paths]
```

```{python}
def read_image(path, img_size=224):
    """Reads and preprocesses an image."""
    try:
        img = load_img(path, color_mode='rgb', target_size=(img_size, img_size))
        img = img_to_array(img)
        img = img / 255.0
        return img
    except Exception as e:
        print(f"Error reading image {path}: {e}")
        return None

def display_images(temp_df, img_dir, num_images=9):
    """Displays a grid of images with their captions."""
    temp_df = temp_df.reset_index(drop=True)
    plt.figure(figsize=(15, 15))
    for i in range(num_images):
        plt.subplot(3, 3, i + 1)
        image_path = os.path.join(img_dir, temp_df.image[i])
        
        image = read_image(image_path)
        if image is not None:
            plt.imshow(image)
            plt.title("\n".join(wrap(temp_df.caption[i], 20)))
        else:
            plt.imshow(np.ones((224, 224, 3))) 
            plt.title("Image not found")
        plt.axis("off")
    plt.tight_layout()
    plt.show()

display_images(data.sample(9), image_path)
```

```{python}
def preprocess_image(img_path, img_size=224):
    """Loads and preprocesses an image for the VGG16 model."""
    img = load_img(img_path, target_size=(img_size, img_size))
    img = img_to_array(img)
    img = preprocess_input(img) 
    return img

def extract_features_batch(image_dir, model, img_size=224, batch_size=32):
    """Extract features from images using a pre-trained model in batches."""
    features = {}
    image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir)]
    
    for i in tqdm(range(0, len(image_paths), batch_size), desc="Extracting features"):
        batch_paths = image_paths[i:i + batch_size]
        batch_images = []

        for img_path in batch_paths:
            img = preprocess_image(img_path, img_size)
            batch_images.append(img)

        batch_images = np.array(batch_images)
        batch_features = model.predict(batch_images, verbose=0)

        for img_path, feature in zip(batch_paths, batch_features):
            img_name = os.path.basename(img_path)
            features[img_name] = feature.flatten()

    return features


def display_images(temp_df, img_dir, num_images=9):
    """Displays a grid of images with their captions."""
    temp_df = temp_df.reset_index(drop=True)
    plt.figure(figsize=(15, 15))
    for i in range(num_images):
        plt.subplot(3, 3, i + 1)
        image_path = os.path.join(img_dir, temp_df.image[i])
        image = preprocess_image(image_path, img_size=224) 
        plt.imshow(image / 255.0)  
        plt.title("\n".join(wrap(temp_df.caption[i], 20)))
        plt.axis("off")
    plt.tight_layout()
    plt.show()

# Display 9 random images with captions
display_images(data.sample(9), image_path)
```

```{python}
def display_caption_stats(captions):
    """Displays some statistics about the captions."""
    caption_lengths = captions.apply(lambda x: len(x.split()))
    print(f"Total captions: {len(captions)}")
    print(f"Average caption length: {caption_lengths.mean():.2f} words")
    print(f"Max caption length: {caption_lengths.max()} words")
    print(f"Min caption length: {caption_lengths.min()} words")

display_caption_stats(data['caption'])

display_images(data.sample(9), image_path)

```

```{python}
print("Dataset Info:")
data.info()

print("\nMissing Values:")
print(data.isnull().sum())

def display_caption_stats(captions):
    """Calculates and displays statistics about the captions."""
    caption_lengths = captions.apply(lambda x: len(x.split()))  
    print(f"\nTotal captions: {len(captions)}")
    print(f"Average caption length: {caption_lengths.mean():.2f} words")
    print(f"Max caption length: {caption_lengths.max()} words")
    print(f"Min caption length: {caption_lengths.min()} words")

display_caption_stats(data['caption'])

def generate_wordcloud(captions):
    """Generates a word cloud from the captions."""
    all_text = " ".join(captions)
    wordcloud = WordCloud(width=800, height=400, background_color="white").generate(all_text)
    
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.title("Word Cloud of Captions", fontsize=20)
    plt.show()

generate_wordcloud(data['caption'])
```

```{python}
from textblob import TextBlob

def analyze_sentiment(captions):
    """Analyzes the sentiment of captions."""
    sentiments = captions.apply(lambda x: TextBlob(x).sentiment.polarity)
    print(f"Average Sentiment: {sentiments.mean():.2f}")
    print(f"Most Positive Caption: {captions.iloc[sentiments.idxmax()]}")
    print(f"Most Negative Caption: {captions.iloc[sentiments.idxmin()]}")
    return sentiments

data['sentiment'] = analyze_sentiment(data['caption'])
plt.hist(data['sentiment'], bins=20, color='skyblue', edgecolor='black')
plt.title("Sentiment Distribution")
plt.xlabel("Sentiment Polarity")
plt.ylabel("Frequency")
plt.show()
```

```{python}
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Add
from tensorflow.keras.optimizers import Adam

def define_model(vocab_size, max_length, embedding_dim=256):
    """Define the captioning model."""
    image_input = tf.keras.Input(shape=(4096,)) 

    caption_input = tf.keras.Input(shape=(max_length,))

    image_features = Dense(embedding_dim, activation='relu')(image_input)

    caption_embedding = Embedding(vocab_size, embedding_dim)(caption_input)
    caption_lstm = LSTM(256)(caption_embedding)

    merged = Add()([image_features, caption_lstm])
    merged = Dropout(0.5)(merged)

    output = Dense(vocab_size, activation='softmax')(merged)

    model = tf.keras.Model(inputs=[image_input, caption_input], outputs=output)
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
```

```{python}
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Model

def build_vgg16_model(img_size=224):
    """Builds a VGG16 model for feature extraction."""
    base_model = VGG16(include_top=False, input_shape=(img_size, img_size, 3))

    x = base_model.output
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
 
    model = Model(inputs=base_model.input, outputs=x)
    
    return model

vgg16_model = build_vgg16_model()
vgg16_model.summary()
```

```{python}
# %%
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Add
from tensorflow.keras.optimizers import Adam

def define_model(vocab_size, max_length, embedding_dim=256):
    """Defines the image captioning model."""
    image_input = tf.keras.Input(shape=(4096,))
    caption_input = tf.keras.Input(shape=(max_length,))
    
    image_features = Dense(embedding_dim, activation='relu')(image_input)
    caption_embedding = Embedding(vocab_size, embedding_dim)(caption_input)
    caption_lstm = LSTM(256)(caption_embedding)
    
    merged = Add()([image_features, caption_lstm])
    merged = Dropout(0.5)(merged)
    
    output = Dense(vocab_size, activation='softmax')(merged)

    model = Model(inputs=[image_input, caption_input], outputs=output)
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
```


```{python}
# %%
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.models import Model

base_model = VGG16(weights='imagenet')
model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)

features_vgg16 = extract_features_batch(image_path, model)
```

```{python}
# %%
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

def visualize_features_vgg16(features_dict, n_components=2):
    """Visualizes extracted features using PCA."""
    feature_matrix = np.array(list(features_dict.values()))

    pca = PCA(n_components=n_components)
    reduced_features = pca.fit_transform(feature_matrix)
    
    plt.figure(figsize=(8, 6))
    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], alpha=0.5, c='blue')
    plt.title("PCA Visualization of VGG16 Extracted Features", fontsize=16)
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.grid(True)
    plt.show()

visualize_features_vgg16(features_vgg16)
```

```{python}
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def visualize_features_vgg16(features_dict, n_components=2):
    """Visualizes VGG16 extracted features using PCA."""
    feature_matrix = np.array(list(features_dict.values()))
    pca = PCA(n_components=n_components)
    reduced_features = pca.fit_transform(feature_matrix)
    
    plt.figure(figsize=(8, 6))
    plt.scatter(reduced_features[:, 0], reduced_features[:, 1], alpha=0.5, c='blue')
    plt.title("PCA Visualization of VGG16 Extracted Features", fontsize=16)
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.grid(True)
    plt.show()

visualize_features_vgg16(features_vgg16)
```

```{python}
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Add
from tensorflow.keras.optimizers import Adam

def define_captioning_model(vocab_size, max_length, embedding_dim=256):
    """Defines a model for generating captions."""
    image_input = tf.keras.Input(shape=(4096,))

    caption_input = tf.keras.Input(shape=(max_length,))

    image_features = Dense(embedding_dim, activation='relu')(image_input)

    caption_embedding = Embedding(vocab_size, embedding_dim)(caption_input)
    caption_lstm = LSTM(256)(caption_embedding)

    merged = Add()([image_features, caption_lstm])
    merged = Dropout(0.5)(merged)

    output = Dense(vocab_size, activation='softmax')(merged)

    model = tf.keras.Model(inputs=[image_input, caption_input], outputs=output)
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model

vocab_size = 10000 
max_length = 34 
captioning_model = define_captioning_model(vocab_size, max_length)
captioning_model.summary()
```